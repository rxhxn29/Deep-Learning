{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027f6828",
   "metadata": {},
   "source": [
    "# Activation Function \n",
    "activation function are important to understand the output activation function gives final output \n",
    "deciding whether output is true or false \n",
    "## types of activation function\n",
    "1) step function (0 or 1) \n",
    "2) sigmoid function (-1 to 1)\n",
    "3) tanh function \n",
    "4) reLU function is best because it can scale (0 to +infinty)\n",
    "5) leaky  reLU function -1 to +infinty in this function encounter vanishing greadient problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2507f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepfunction(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def sigmoid(x):\n",
    "    import math\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    import math\n",
    "    return math.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return x if x >= 0 else alpha * x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773f64b",
   "metadata": {},
   "source": [
    "# loss and cost function\n",
    "for individuals error we check loss function \n",
    "and for mean error we check cost function\n",
    "it is one of imp concetp for learing machine learing and deeplearing is help in adjusting weight and bised "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for classification tasks\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return sum(abs(t - p) for t, p in zip(y_true, y_pred)) / len(y_true)\n",
    "# useful for regression tasks\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return sum((t - p) ** 2 for t, p in zip(y_true, y_pred)) / len(y_true)\n",
    "# useful for regression tasks\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    import math\n",
    "    return math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "# useful for regression tasks\n",
    "def log_loss(y_true, y_pred):\n",
    "    import math\n",
    "    epsilon = 1e-15\n",
    "    y_pred = [min(max(p, epsilon), 1 - epsilon) for p in y_pred]\n",
    "    return -sum(t * math.log(p) + (1 - t) * math.log(1 - p) for t, p in zip(y_true, y_pred)) / len(y_true)\n",
    "# useful for binary classification tasks\n",
    "def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    import math\n",
    "    epsilon = 1e-15\n",
    "    y_pred = [min(max(p, epsilon), 1 - epsilon) for p in y_pred]\n",
    "    return -sum(math.log(y_pred[t]) for t, p in zip(y_true, y_pred)) / len(y_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306a9e4",
   "metadata": {},
   "source": [
    "# Gradent Decent (heart of neural network)\n",
    "find global minma for weight and bisa for better accuracy score \n",
    "w1= w1 - learing_rate * partial diff wrt w1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbfff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def batch_gradient_descent( X, y, learning_rate=0.01, epochs=1000):\n",
    "        m, n = X.shape\n",
    "        weights = np.zeros(n)\n",
    "        bias = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            weighted_sum = np.dot(X, weights) + bias\n",
    "            y_pred = self.sigmoid(weighted_sum)\n",
    "            log_loss_value = self._log_loss(y, y_pred)\n",
    "            error = y_pred - y\n",
    "            dw = (1/m) * np.dot(np.transpose(X), error)\n",
    "            db = (1/m) * np.sum(error)\n",
    "            weights -= learning_rate * dw\n",
    "            bias -= learning_rate * db\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Log Loss: {log_loss_value}')\n",
    "\n",
    "        return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bdc91",
   "metadata": {},
   "source": [
    "# Single neural network from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af7efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.weights = 1\n",
    "        self.bias = 0\n",
    "\n",
    "    def fit(self, X, y, method='batch', learning_rate=0.01, epochs=1000, loss_function='log_loss'):\n",
    "        if method == 'batch':\n",
    "            return self.batch_gradient_descent(X, y, learning_rate, epochs)\n",
    "        elif method == 'stochastic':\n",
    "            return self.stochastic_gradient_descent(X, y, learning_rate, epochs)\n",
    "        elif method == 'momentum':\n",
    "            return self.momentum_gradient_descent(X, y, learning_rate, epochs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose 'batch', 'stochastic', or 'momentum'.\")\n",
    "\n",
    "    def stepfunction(self, x):\n",
    "        return np.where(np.asarray(x) >= 0, 1, 0)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def leaky_relu(self, x, alpha=0.01):\n",
    "        x = np.asarray(x)\n",
    "        return np.where(x >= 0, x, alpha * x)\n",
    "\n",
    "    def _log_loss(self, y_true, y_pred, eps=1e-15):\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def batch_gradient_descent(self, X, y, learning_rate=0.01, epochs=1000):\n",
    "        m, n = X.shape\n",
    "        weights = np.zeros(n)\n",
    "        bias = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            weighted_sum = np.dot(X, weights) + bias\n",
    "            y_pred = self.sigmoid(weighted_sum)\n",
    "            log_loss_value = self._log_loss(y, y_pred)\n",
    "            error = y_pred - y\n",
    "            dw = (1/m) * np.dot(np.transpose(X), error)\n",
    "            db = (1/m) * np.sum(error)\n",
    "            weights -= learning_rate * dw\n",
    "            bias -= learning_rate * db\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Log Loss: {log_loss_value}')\n",
    "\n",
    "        return weights, bias\n",
    "\n",
    "    def stochastic_gradient_descent(self, X, y, learning_rate=0.01, epochs=1000):\n",
    "        m, n = X.shape\n",
    "        weights = np.zeros(n)\n",
    "        bias = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(m):\n",
    "                linear_output = np.dot(X[i], weights) + bias\n",
    "                y_pred = self.sigmoid(linear_output)\n",
    "                error = y_pred - y[i]\n",
    "\n",
    "                dw = error * X[i]\n",
    "                db = error\n",
    "\n",
    "                weights -= learning_rate * dw\n",
    "                bias -= learning_rate * db\n",
    "\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Sample {i+1}/{m}')\n",
    "\n",
    "        return weights, bias\n",
    "\n",
    "    def momentum_gradient_descent(self, X, y, learning_rate=0.01, epochs=1000, beta=0.9):\n",
    "        m, n = X.shape\n",
    "        weights = np.zeros(n)\n",
    "        bias = 0\n",
    "        vw = np.zeros(n)\n",
    "        vb = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = np.dot(X, weights) + bias\n",
    "            y_prob = self.sigmoid(y_pred)\n",
    "            error = y_prob - y\n",
    "\n",
    "            dw = (1/m) * np.dot(X.T, error)\n",
    "            db = (1/m) * np.sum(error)\n",
    "\n",
    "            vw = beta * vw + (1 - beta) * dw\n",
    "            vb = beta * vb + (1 - beta) * db\n",
    "\n",
    "            weights -= learning_rate * vw\n",
    "            bias -= learning_rate * vb\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "        return weights, bias\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
